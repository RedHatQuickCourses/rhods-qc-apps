{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97b1d6d2-1a32-4a8e-b125-5ea8b389b85d",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "Run this notebook to train a basic neural network by using the preprocessed data and the Keras high-level TensorFlow API.\n",
    "\n",
    "At the bottom of the notebook, after model training, add the code to convert the trained model to ONNX format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdb7bbc-5cc5-4045-b2c6-b98afce790fc",
   "metadata": {},
   "source": [
    "First, import the necessary libraries and deactivate GPU acceleration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e40e8d5c-f870-43df-8968-905f8370c515",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:16:28.425790: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-08 15:16:30.218302: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-11-08 15:16:30.218385: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-11-08 15:16:30.218395: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from os import environ\n",
    "\n",
    "environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import Adam\n",
    "from numpy import load\n",
    "from onnx import save\n",
    "from tf2onnx import convert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698b05ea-47b2-4b2b-8e12-de857916ec14",
   "metadata": {},
   "source": [
    "Next, create a basic neural network, and train the network by using the preprocessed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "993bd9e1-04af-480a-bd22-c3ba9a23c4d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training model\n",
      "Epoch 1/20\n",
      "1214/1214 - 2s - loss: 0.0726 - accuracy: 0.9718 - val_loss: 0.0263 - val_accuracy: 0.9920 - 2s/epoch - 2ms/step\n",
      "Epoch 2/20\n",
      "1214/1214 - 2s - loss: 0.0136 - accuracy: 0.9968 - val_loss: 0.0053 - val_accuracy: 1.0000 - 2s/epoch - 1ms/step\n",
      "Epoch 3/20\n",
      "1214/1214 - 2s - loss: 0.0070 - accuracy: 0.9986 - val_loss: 0.0097 - val_accuracy: 0.9995 - 2s/epoch - 1ms/step\n",
      "Epoch 4/20\n",
      "1214/1214 - 2s - loss: 0.0046 - accuracy: 0.9991 - val_loss: 0.0019 - val_accuracy: 1.0000 - 2s/epoch - 1ms/step\n",
      "Epoch 5/20\n",
      "1214/1214 - 2s - loss: 0.0037 - accuracy: 0.9993 - val_loss: 0.0029 - val_accuracy: 1.0000 - 2s/epoch - 1ms/step\n",
      "Epoch 6/20\n",
      "1214/1214 - 2s - loss: 0.0030 - accuracy: 0.9994 - val_loss: 0.0018 - val_accuracy: 0.9997 - 2s/epoch - 1ms/step\n",
      "Epoch 7/20\n",
      "1214/1214 - 2s - loss: 0.0025 - accuracy: 0.9995 - val_loss: 0.0027 - val_accuracy: 1.0000 - 2s/epoch - 1ms/step\n",
      "Epoch 8/20\n",
      "1214/1214 - 2s - loss: 0.0023 - accuracy: 0.9996 - val_loss: 8.7773e-04 - val_accuracy: 1.0000 - 2s/epoch - 1ms/step\n",
      "Epoch 9/20\n",
      "1214/1214 - 2s - loss: 0.0020 - accuracy: 0.9996 - val_loss: 0.0013 - val_accuracy: 1.0000 - 2s/epoch - 1ms/step\n",
      "Epoch 10/20\n",
      "1214/1214 - 2s - loss: 0.0019 - accuracy: 0.9996 - val_loss: 2.2378e-04 - val_accuracy: 1.0000 - 2s/epoch - 1ms/step\n",
      "Epoch 11/20\n",
      "1214/1214 - 2s - loss: 0.0015 - accuracy: 0.9997 - val_loss: 0.0097 - val_accuracy: 0.9968 - 2s/epoch - 1ms/step\n",
      "Epoch 12/20\n",
      "1214/1214 - 2s - loss: 0.0016 - accuracy: 0.9997 - val_loss: 3.1088e-04 - val_accuracy: 1.0000 - 2s/epoch - 1ms/step\n",
      "Epoch 13/20\n",
      "1214/1214 - 2s - loss: 0.0014 - accuracy: 0.9998 - val_loss: 7.1644e-04 - val_accuracy: 1.0000 - 2s/epoch - 1ms/step\n",
      "Epoch 14/20\n",
      "1214/1214 - 2s - loss: 0.0012 - accuracy: 0.9997 - val_loss: 2.2249e-04 - val_accuracy: 1.0000 - 2s/epoch - 1ms/step\n",
      "Epoch 15/20\n",
      "1214/1214 - 2s - loss: 0.0012 - accuracy: 0.9998 - val_loss: 2.4286e-04 - val_accuracy: 1.0000 - 2s/epoch - 1ms/step\n",
      "Epoch 16/20\n",
      "1214/1214 - 2s - loss: 0.0012 - accuracy: 0.9998 - val_loss: 0.0025 - val_accuracy: 0.9993 - 2s/epoch - 1ms/step\n",
      "Epoch 17/20\n",
      "1214/1214 - 2s - loss: 9.9806e-04 - accuracy: 0.9998 - val_loss: 3.8181e-04 - val_accuracy: 1.0000 - 2s/epoch - 1ms/step\n",
      "Epoch 18/20\n",
      "1214/1214 - 2s - loss: 0.0011 - accuracy: 0.9998 - val_loss: 3.5925e-04 - val_accuracy: 1.0000 - 2s/epoch - 1ms/step\n",
      "Epoch 19/20\n",
      "1214/1214 - 2s - loss: 7.9881e-04 - accuracy: 0.9999 - val_loss: 1.0618e-04 - val_accuracy: 1.0000 - 2s/epoch - 1ms/step\n",
      "Epoch 20/20\n",
      "1214/1214 - 2s - loss: 8.3637e-04 - accuracy: 0.9998 - val_loss: 2.0100e-04 - val_accuracy: 1.0000 - 2s/epoch - 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fabf803a6d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('training model')\n",
    "\n",
    "epoch_count = int(environ.get('epoch_count', '20'))\n",
    "learning_rate = float(environ.get('learning_rate', '0.001'))\n",
    "\n",
    "Xsm_train = load(f'data/training_samples.npy')\n",
    "ysm_train = load(f'data/training_labels.npy')\n",
    "n_inputs = Xsm_train.shape[1]\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(n_inputs, input_shape=(n_inputs, ), activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(2, activation='softmax'),\n",
    "])\n",
    "model.compile(\n",
    "    Adam(learning_rate=learning_rate),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "model.fit(\n",
    "    Xsm_train,\n",
    "    ysm_train,\n",
    "    validation_split=0.2,\n",
    "    batch_size=300,\n",
    "    epochs=epoch_count,\n",
    "    shuffle=True,\n",
    "    verbose=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cf108f-c4b7-4113-95a8-401105e7d09e",
   "metadata": {},
   "source": [
    "# Export the model to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e70bb8cc-c9e8-4fcc-9cd7-9e5d288b564e",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model, _ = convert.from_keras(model)\n",
    "save(onnx_model, 'model.onnx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
