{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Retraining\n",
    "\n",
    "We will now try to load the model that we created in the previous lab and retrain it by changing some hyper parameters to get better accuracy\n",
    "\n",
    "To start we will need to import our required libraries and packages.  We will also load our diabetes data set, create test and training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diabetes model using PyTorch\n",
    "# Uses the data file:  diabetes.csv\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have imported our required libraries and packages we load the data into a dataframe (df)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code assumes that the file is in a data folder up one dir from this file.\n",
    "data_file_name = '../data/diabetes.csv'\n",
    "model_saved_name = '../model/PytorchDiabetesModel.pt'\n",
    "\n",
    "df = pd.read_csv(data_file_name)\n",
    "X = df.drop('Outcome', axis=1)  # Independent Feature\n",
    "y = df['Outcome']                 # Dependent Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can re-train the model, we do the same data processing as before to maintain consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Creating Tensors (multidimensional matrix) x-input data  y-output data\n",
    "X_train = torch.FloatTensor(X_train.values)\n",
    "X_test = torch.FloatTensor(X_test.values)\n",
    "y_train = torch.LongTensor(y_train.values)\n",
    "y_test = torch.LongTensor(y_test.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load our model and try to retrain it by changing the epochs hyper parameter. An epoch defines the number of times the entire data set has to be worked through the learning algorithm. Every sample in the training dataset has had a chance to update the internal model parameters once during an epoch\n",
    "\n",
    "So the intent behind increasing the number of epochs is that the algorithm has more time to train itself and it can make better predictions on data that it has not seen before!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN_model(nn.Module):\n",
    "    def __init__(self, input_features=8, hidden1=20, hidden2=10, out_features=2):\n",
    "        super().__init__()\n",
    "        self.f_connected1 = nn.Linear(input_features, hidden1)\n",
    "        self.f_connected2 = nn.Linear(hidden1, hidden2)\n",
    "        self.out = nn.Linear(hidden2, out_features)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.f_connected1(x))\n",
    "        x = F.relu(self.f_connected2(x))\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "    def save(self, model_path):\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    def load(self, model_path):\n",
    "        self.load_state_dict(torch.load(model_path))\n",
    "        self.eval()\n",
    "\n",
    "\n",
    "torch.manual_seed(20)\n",
    "\n",
    "model = ANN_model()\n",
    "model.load(model_saved_name)\n",
    "\n",
    "\n",
    "# Backward Propagation - loss and optimizer\n",
    "loss_function = nn.CrossEntropyLoss()  # CrossEntropyLoss also used in Tensorflow\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=0.01)  # Tensorflow also uses Adam\n",
    "\n",
    "epochs = 1000\n",
    "final_losses = []\n",
    "for i in range(epochs):\n",
    "    i = i+1\n",
    "    y_pred = model.forward(X_train)\n",
    "    loss = loss_function(y_pred, y_train)\n",
    "    final_losses.append(loss)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once our model is created we should test the model's accuracy.  We can do this by comparing the results from the test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7012987012987013\n"
     ]
    }
   ],
   "source": [
    "# Accuracy - comparing the results from the test data\n",
    "\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(X_test):\n",
    "        y_pred = model(data)\n",
    "        predictions.append(y_pred.argmax())\n",
    "   \n",
    "score = accuracy_score(y_test, predictions)  # Simply calculates number of hits / length of y_test\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that even if we trained our model for double the time that it trained on previously, it still produced an accuracy that was lower than before! This phenomenon is called as overfitting.\n",
    "\n",
    "You only get accurate predictions if the machine learning model generalizes to all types of data within its domain. Overfitting occurs when the model cannot generalize and fits too closely to the training dataset instead. Overfitting happens due to several reasons, such as:\n",
    "\n",
    "    • The training data size is too small and does not contain enough data samples to accurately represent all possible input data values.\n",
    "\n",
    "    • The training data contains large amounts of irrelevant information, called noisy data.\n",
    "\n",
    "    • The model trains for too long on a single sample set of data.\n",
    "    \n",
    "    • The model complexity is high, so it learns the noise within the training data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
