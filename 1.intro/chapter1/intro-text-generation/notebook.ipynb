{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbf6813a-573a-4adb-8dd4-63c9adb000a3",
   "metadata": {},
   "source": [
    "# Text Generation Demo\n",
    "\n",
    "This demo shows you how to fine-tune a pretrained language model for text generation by using _Causal language modeling_.\n",
    "Fine-tuning is the process of training a pretrained AI model on a specific task or dataset to adapt the model to your needs.\n",
    "In this way, you refine the performance of the model for your specific use case, without having to retrain the model from scratch.\n",
    "\n",
    "Causal language modeling is a natural language processing technique that predicts the next token of a sequence of tokens, and it's typically used for text generation.\n",
    "Large language models (LLMs) such as Llama2 and GPT-4 have shown splendid results in text generation.\n",
    "However, the size of this models require vasts amounts of computational and memory resources for training, and even for fine-tuning the pretrained models.\n",
    "\n",
    "Instead, this demo fine tunes the DistilGPT-2 model, which is a smaller model developed by Hugging Face.\n",
    "Note that, altough the model is smaller, the fine-tuning step still might take hours if you do not have access to a GPU.\n",
    "For that reason, this notebook deactivates the training phase by default, and instead provides the final fine tuned model.\n",
    "If you wish to change this behaviour, change the following variable to `True`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95ec0471-1955-4d64-8a47-2dafc31ed473",
   "metadata": {},
   "outputs": [],
   "source": [
    "DO_TRAIN = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61cbd4a-ba0f-4fc3-ad7b-2ce660381861",
   "metadata": {},
   "source": [
    "First, install the dependencies that are required for this demo and are not installed in the PyTorch workbench.\n",
    "This demo uses the `transformers` library ecosystem, which is a common choice when training language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e19bc14-6e1d-4238-b321-400fc97d081d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers[torch]==4.34.0\n",
      "  Downloading transformers-4.34.0-py3-none-any.whl (7.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m146.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting datasets==2.14.5\n",
      "  Downloading datasets-2.14.5-py3-none-any.whl (519 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.6/519.6 kB\u001b[0m \u001b[31m335.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting evaluate==0.4.0\n",
      "  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m275.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /opt/app-root/lib/python3.9/site-packages (from transformers[torch]==4.34.0) (6.0.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/app-root/lib/python3.9/site-packages (from transformers[torch]==4.34.0) (4.66.1)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2023.10.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m773.3/773.3 kB\u001b[0m \u001b[31m333.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting safetensors>=0.3.1\n",
      "  Downloading safetensors-0.4.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m314.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers<0.15,>=0.14\n",
      "  Downloading tokenizers-0.14.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m286.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/app-root/lib/python3.9/site-packages (from transformers[torch]==4.34.0) (1.24.4)\n",
      "Requirement already satisfied: requests in /opt/app-root/lib/python3.9/site-packages (from transformers[torch]==4.34.0) (2.31.0)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4\n",
      "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m320.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting filelock\n",
      "  Downloading filelock-3.12.4-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/app-root/lib/python3.9/site-packages (from transformers[torch]==4.34.0) (23.1)\n",
      "Collecting accelerate>=0.20.3\n",
      "  Downloading accelerate-0.23.0-py3-none-any.whl (258 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.1/258.1 kB\u001b[0m \u001b[31m316.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch!=1.12.0,>=1.10 in /opt/app-root/lib/python3.9/site-packages (from transformers[torch]==4.34.0) (1.13.1)\n",
      "Collecting pyarrow>=8.0.0\n",
      "  Downloading pyarrow-13.0.0-cp39-cp39-manylinux_2_28_x86_64.whl (40.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.1/40.1 MB\u001b[0m \u001b[31m243.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting fsspec[http]<2023.9.0,>=2023.1.0\n",
      "  Downloading fsspec-2023.6.0-py3-none-any.whl (163 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/163.8 kB\u001b[0m \u001b[31m329.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting xxhash\n",
      "  Downloading xxhash-3.4.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (193 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.8/193.8 kB\u001b[0m \u001b[31m308.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/app-root/lib/python3.9/site-packages (from datasets==2.14.5) (0.3.7)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.15-py39-none-any.whl (133 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.3/133.3 kB\u001b[0m \u001b[31m287.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas in /opt/app-root/lib/python3.9/site-packages (from datasets==2.14.5) (1.5.3)\n",
      "Requirement already satisfied: aiohttp in /opt/app-root/lib/python3.9/site-packages (from datasets==2.14.5) (3.8.5)\n",
      "Collecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: psutil in /opt/app-root/lib/python3.9/site-packages (from accelerate>=0.20.3->transformers[torch]==4.34.0) (5.9.5)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/app-root/lib/python3.9/site-packages (from aiohttp->datasets==2.14.5) (23.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/app-root/lib/python3.9/site-packages (from aiohttp->datasets==2.14.5) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/app-root/lib/python3.9/site-packages (from aiohttp->datasets==2.14.5) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/app-root/lib/python3.9/site-packages (from aiohttp->datasets==2.14.5) (1.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/app-root/lib/python3.9/site-packages (from aiohttp->datasets==2.14.5) (6.0.4)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/app-root/lib/python3.9/site-packages (from aiohttp->datasets==2.14.5) (3.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/app-root/lib/python3.9/site-packages (from aiohttp->datasets==2.14.5) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/app-root/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers[torch]==4.34.0) (4.7.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/app-root/lib/python3.9/site-packages (from requests->transformers[torch]==4.34.0) (1.26.16)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/app-root/lib/python3.9/site-packages (from requests->transformers[torch]==4.34.0) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/app-root/lib/python3.9/site-packages (from requests->transformers[torch]==4.34.0) (2023.7.22)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /opt/app-root/lib/python3.9/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]==4.34.0) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /opt/app-root/lib/python3.9/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]==4.34.0) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /opt/app-root/lib/python3.9/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]==4.34.0) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /opt/app-root/lib/python3.9/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]==4.34.0) (8.5.0.96)\n",
      "Requirement already satisfied: wheel in /opt/app-root/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch!=1.12.0,>=1.10->transformers[torch]==4.34.0) (0.38.4)\n",
      "Requirement already satisfied: setuptools in /opt/app-root/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch!=1.12.0,>=1.10->transformers[torch]==4.34.0) (68.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/app-root/lib/python3.9/site-packages (from pandas->datasets==2.14.5) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/app-root/lib/python3.9/site-packages (from pandas->datasets==2.14.5) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/app-root/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->datasets==2.14.5) (1.16.0)\n",
      "Installing collected packages: xxhash, safetensors, regex, pyarrow, multiprocess, fsspec, filelock, responses, huggingface-hub, tokenizers, transformers, datasets, accelerate, evaluate\n",
      "Successfully installed accelerate-0.23.0 datasets-2.14.5 evaluate-0.4.0 filelock-3.12.4 fsspec-2023.6.0 huggingface-hub-0.17.3 multiprocess-0.70.15 pyarrow-13.0.0 regex-2023.10.3 responses-0.18.0 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.34.0 xxhash-3.4.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers[torch]==4.34.0 datasets==2.14.5 evaluate==0.4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9c0b8e-e45a-4c2f-b640-276b3c1bee90",
   "metadata": {},
   "source": [
    "Import the dependencies for the exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9dfbc87-cb5b-46a0-aecb-4df54c5108a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from datasets import Dataset\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    pipeline, AutoTokenizer, DataCollatorForLanguageModeling, \n",
    "    AutoModelForCausalLM, TrainingArguments, Trainer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f00e40-7e2f-475d-be42-6b4dcf091ff4",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "The demo fine-tunes the DistilGPT-2 model to better generate text related to Open Data Hub.\n",
    "To this end, the demo provides a subset of the asciidoc source code of the Open Data Hub Documentation in the `odh-merged-docs.adoc` file.\n",
    "The complete documentation is available at https://github.com/opendatahub-io/opendatahub-documentation.\n",
    "\n",
    "You can use this data to _teach_ the model how to write more Open Data Hub content.\n",
    "\n",
    "Load the data with the `datasets` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f2e88c9-8c48-4405-84cf-d98780e2a9fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 3793\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = load_dataset(\"text\", data_files={\"data\": \"odh-merged-docs.adoc\"}, split=\"data\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0b1979-8b7f-412a-97dc-e4506af156b5",
   "metadata": {},
   "source": [
    "## Create the Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805537f3-5be2-4084-a567-d82f2c225b4a",
   "metadata": {},
   "source": [
    "Create a tokenizer.\n",
    "A tokenizer is a key component of language models.\n",
    "It converts raw text into numerical ids (tokens) that can be processed by the neural network inside the model.\n",
    "\n",
    "In this case, use the the tokenizer that is specific for the DistilGPT-2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ead14f84-62f4-4aff-a51e-bc96486a32af",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658512c7-f2b9-45ba-b461-5e8ba2617434",
   "metadata": {},
   "source": [
    "You can test the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddfbd995-02eb-4de8-8d32-8ca08208b837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [15496, 995, 0], 'attention_mask': [1, 1, 1]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"Hello world!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592d7a89-6714-4fe1-8510-d389eb653f33",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "Preprocess the data by tokenizing the text and grouping the samples in batches.\n",
    "You must also divide the data into training and testing splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd192497-cfaa-46a3-bbba-a30f6f947171",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "809b7915a0c64730a821a0d9b9ccfad0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/3034 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (7068 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1915 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1790 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (7814 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85aa61fc978247e7847f4d6a4448632a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/759 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (5457 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1687 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3206 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4385 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "def preprocess_function(samples):\n",
    "    return tokenizer([f\"{x}\\n\".join(x) for x in samples[\"text\"]])\n",
    "\n",
    "ds = ds.train_test_split(test_size=0.2)\n",
    "\n",
    "tokenized_ds = ds.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    remove_columns=ds[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f265f80-7124-4e47-8a06-5209e82de15f",
   "metadata": {},
   "source": [
    "Inspect the dataset and verify that two subsets are included now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b6b9af6-34cf-4824-a083-bc20c5088473",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 3034\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 759\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064d3d58-1027-44e9-a0cb-96bf601eaa27",
   "metadata": {},
   "source": [
    "Concatenate all the token sequences and chunk them into blocks.\n",
    "\n",
    "This is important to ensure that every block of tokens that we use for training fits in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c4c9aa6-c248-4f7e-bd68-a75ba85811e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03074094df844a68881cdcbc65fe1a48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/3034 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2c9e028f5b541f5b49076b92ed8ef81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/759 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 21154\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 6194\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 256\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported \n",
    "    # it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    if total_length >= block_size:\n",
    "        total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of block_size.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "lm_dataset = tokenized_ds.map(group_texts, batched=True, num_proc=4)\n",
    "lm_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab21f966-de33-420b-9271-b89f1d5ceb34",
   "metadata": {},
   "source": [
    "Finally, define the data collation and the padding strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b670cd10-17b6-4e7c-8943-a1452534a3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595b03c8-fc1c-415b-94db-64b7589abb9c",
   "metadata": {},
   "source": [
    "## Training (fine-tuning)\n",
    "\n",
    "Load the pretrained base DistilGPT-2 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "779e0dda-579e-4885-96e1-73db8ba87d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7a6356-7f23-4b26-aeca-f47eb74697a0",
   "metadata": {},
   "source": [
    "Train the model with your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "391cd1ff-f4d2-465c-9982-e51a02da89be",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DO_TRAIN:\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"my_model\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        learning_rate=2e-5,\n",
    "        num_train_epochs=1,\n",
    "        weight_decay=0.01\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=lm_dataset[\"train\"],\n",
    "        eval_dataset=lm_dataset[\"test\"],\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fad29b-f417-432a-8966-bcc7e244b5e4",
   "metadata": {},
   "source": [
    "Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9475809a-ffff-4d58-bd9c-1ad2803fbaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DO_TRAIN:\n",
    "    eval_results = trainer.evaluate()\n",
    "    print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed63f90-e63b-4ad8-bdc4-68ad64062bfb",
   "metadata": {},
   "source": [
    "# Testing\n",
    "\n",
    "\n",
    "## Download Model (ONLY if `DO_TRAIN` is False)\n",
    "If `DO_TRAIN` is False, you need to download the model before testing it.\n",
    "\n",
    "Download the [pytorch_model.bin](https://drive.google.com/file/d/142H5pfiw7JKN29xv9rav1ZTDF2a8MCoZ/view?usp=sharing) from Google Drive file into your computer.\n",
    "Then, upload the file into the `my_model/` directory of this workspace.\n",
    "\n",
    "Wait for the file to upload.\n",
    "You can verify the upload progress in the tool bar at the bottom of the screen.\n",
    "After you have uploaded the file, verify that the file is in the `my_model` directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c25b565f-b034-41c2-85dc-8e0f43d0f73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--. 1 1003310000 1003310000 327674773 Oct  9 08:54 my_model/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "%ls -l my_model/pytorch_model.bin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f0cf8a-a2f3-491b-81c1-b941dec86cd9",
   "metadata": {},
   "source": [
    "## Run the Tests\n",
    "Generate text given the following prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a4ae53d3-541f-4cb5-9eff-debe08fc00e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Use Elyra to\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01518676-29a6-449f-85e4-590754ec68b1",
   "metadata": {},
   "source": [
    "First, verify the text produced by the base DistilGPT-2 model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2f24172b-070f-4e89-9247-215ce1db11d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use Elyra to the Moon\n",
      "\n",
      "There's going to be a lot of time before we get to that point. I've been going on for a couple of seasons before, and here's the main point: the most important thing about that story\n"
     ]
    }
   ],
   "source": [
    "base_generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "print(base_generator(prompt)[0][\"generated_text\"].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e9f63d-d2a3-4959-8e72-c9d5a281df1b",
   "metadata": {},
   "source": [
    "Now, test the text generated by the fine tuned model.\n",
    "The output might sound closer to the OpenDataHub docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "277a403e-f652-4be9-a2f5-60b8ef602c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use Elyra to create visual end-to-end pipelines that easily run pipelines across your notebook server. Elyra is an extension for JupyterLab that provides you with a Pipeline Editor to create pipeline workflows that can be executed in {\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline(\"text-generation\", model=\"./my_model\", tokenizer=tokenizer)\n",
    "print(generator(prompt)[0][\"generated_text\"].strip())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
